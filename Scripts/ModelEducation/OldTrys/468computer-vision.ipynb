{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:09:48.804800Z","iopub.status.busy":"2024-02-03T13:09:48.804428Z","iopub.status.idle":"2024-02-03T13:10:05.204028Z","shell.execute_reply":"2024-02-03T13:10:05.203023Z","shell.execute_reply.started":"2024-02-03T13:09:48.804769Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: imutils in /opt/homebrew/lib/python3.11/site-packages (0.5.4)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip3 install imutils"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:57:00.256641Z","iopub.status.busy":"2024-02-03T13:57:00.256264Z","iopub.status.idle":"2024-02-03T13:57:00.261861Z","shell.execute_reply":"2024-02-03T13:57:00.260854Z","shell.execute_reply.started":"2024-02-03T13:57:00.256610Z"},"trusted":true},"outputs":[],"source":["import joblib\n","\n","# Save the model\n","def save_model(model, path):\n","    #joblib.dump(svm_clf_dct, 'svm_dct_model.pkl')\n","    joblib.dump(model, path)\n","    \n","def load_model(path):\n","    # Load the model\n","    #svm_clf_dct_local = joblib.load('/kaggle/input/svm-dct-model/svm_dct_model.pkl')\n","    return(joblib.load(path))\n"]},{"cell_type":"markdown","metadata":{},"source":["# **CLASSICAL METHODS PART**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:10:05.206380Z","iopub.status.busy":"2024-02-03T13:10:05.206063Z","iopub.status.idle":"2024-02-03T13:10:05.614906Z","shell.execute_reply":"2024-02-03T13:10:05.613941Z","shell.execute_reply.started":"2024-02-03T13:10:05.206352Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/4s/9lp6822x67n5q20pk553klcw0000gn/T/ipykernel_95644/18250641.py:26: DeprecationWarning: Please use `QhullError` from the `scipy.spatial` namespace, the `scipy.spatial.qhull` namespace is deprecated.\n","  from scipy.spatial.qhull import QhullError\n"]}],"source":["import cv2\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from tqdm import tqdm  # for progress bar\n","import random \n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","\n","from imutils import paths\n","from matplotlib import pyplot as plt\n","import matplotlib.image as mpimg\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score \n","from skimage.filters import gabor\n","from scipy.spatial.qhull import QhullError\n","from scipy import spatial\n","spatial.QhullError = QhullError\n","from skimage.feature import local_binary_pattern\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:10:05.616479Z","iopub.status.busy":"2024-02-03T13:10:05.616199Z","iopub.status.idle":"2024-02-03T13:10:05.621486Z","shell.execute_reply":"2024-02-03T13:10:05.620501Z","shell.execute_reply.started":"2024-02-03T13:10:05.616455Z"},"trusted":true},"outputs":[],"source":["def colortogray(im):\n","    image = cv2.imread(im)\n","    imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    return imgray\n","\n","def resizeImage(image, size):\n","    return cv2.resize(image, (size,size))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:10:05.623904Z","iopub.status.busy":"2024-02-03T13:10:05.623642Z","iopub.status.idle":"2024-02-03T13:10:05.638162Z","shell.execute_reply":"2024-02-03T13:10:05.637310Z","shell.execute_reply.started":"2024-02-03T13:10:05.623876Z"},"trusted":true},"outputs":[],"source":["def extract_features_labels(directory):\n","    features = []\n","    labels = []\n","    for emotion_folder in os.listdir(directory):\n","        emotion_folder_path = os.path.join(directory, emotion_folder)\n","        \n","        # Check if it's a directory\n","        if os.path.isdir(emotion_folder_path):\n","            print(f\"Processing {emotion_folder} images...\")\n","        for image_file in tqdm(os.listdir(emotion_folder_path)):\n","                image_path = os.path.join(emotion_folder_path, image_file)\n","                image = colortogray(image_path)\n","                image = resizeImage(image, 64)\n","\n","                fd1 = fd1 =  hog(image, orientations=7, pixels_per_cell=(8, 8),cells_per_block=(4, 4),block_norm= 'L2-Hys' ,transform_sqrt = False)\n","                label = emotion_folder\n","                labels.append(label)\n","                features.append(fd1)\n","    features = np.array(features)\n","    labels = np.array(labels)\n","    return features,labels"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:10:05.639796Z","iopub.status.busy":"2024-02-03T13:10:05.639534Z","iopub.status.idle":"2024-02-03T13:10:05.658026Z","shell.execute_reply":"2024-02-03T13:10:05.657320Z","shell.execute_reply.started":"2024-02-03T13:10:05.639774Z"},"trusted":true},"outputs":[],"source":["from scipy.fftpack import dct, idct\n","\n","# implement 2D DCT\n","def dct2_method(a):\n","    return dct(dct(a.T, norm='ortho').T, norm='ortho')\n","\n","# implement 2D IDCT\n","def idct2_method(a):\n","    return idct(idct(a.T, norm='ortho').T, norm='ortho')\n","\n","def lbp_method(image, P, R):\n","    lbp = local_binary_pattern(image, P, R, method=\"uniform\")\n","    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, P + 3), range=(0, P + 2))\n","    hist = hist.astype(\"float\")\n","    hist /= (hist.sum() + 1e-7)\n","    return hist\n","\n","def gabor_filters_method(image, frequencies):\n","    responses = []\n","    for frequency in frequencies:\n","        real, imag = gabor(image, frequency=frequency)\n","        magnitude = np.sqrt(real**2 + imag**2)\n","        responses.append(magnitude.flatten())  # Flatten each response\n","    combined_response = np.concatenate(responses, axis=0)  # Concatenate flattened responses\n","    return combined_response\n","\n","def merge_features(*feature_arrays):\n","    for i in range(1, len(feature_arrays)):\n","        if feature_arrays[i].shape[0] != feature_arrays[0].shape[0]:\n","            raise ValueError(\"Tüm özellik dizileri aynı sayıda örneğe sahip olmalıdır.\")\n","\n","    # Numpy'ın hstack fonksiyonu ile özellikler yatay olarak birleştirilir\n","    combined_features = np.hstack(feature_arrays)\n","    return combined_features"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:26:08.899172Z","iopub.status.busy":"2024-02-03T13:26:08.898803Z","iopub.status.idle":"2024-02-03T13:26:08.911137Z","shell.execute_reply":"2024-02-03T13:26:08.910306Z","shell.execute_reply.started":"2024-02-03T13:26:08.899143Z"},"trusted":true},"outputs":[],"source":["# DISPLAY METHODS\n","\n","import math\n","emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","\n","def view_random_images(directory, target_class, num_of_samples=8):\n","  target_folder = os.path.join(directory, target_class)\n","\n","  random_images = random.sample(os.listdir(target_folder), num_of_samples)\n","  print(random_images)\n","  # Create a subplot to display multiple images\n","  plt.figure(figsize=(12, 6))\n","  for i, image in enumerate(random_images):\n","    plt.subplot(math.ceil(num_of_samples / 4), 4, i + 1)  # 2 rows, 4 columns\n","    img = cv2.imread(os.path.join(target_folder, image))\n","    \n","    plt.imshow(img)\n","    plt.title(f\"{target_class} - {i + 1}\")\n","    plt.axis(\"off\")\n","\n","  plt.show()\n","\n","def get_sample_of_images(directory, num_of_samples = 8):    \n","    for emotion in emotions:\n","        view_random_images(directory, emotion, num_of_samples)\n","\n","def get_sample_of_feature_extractions(directory, num_of_samples = 8, method_name = 'dct'):\n","    for emotion_target in emotions:\n","        target_folder = os.path.join(directory, emotion_target)\n","        random_images = random.sample(os.listdir(target_folder), num_of_samples)\n","        print(emotion_target)\n","        for i, image in enumerate(random_images):\n","            img = cv2.imread(os.path.join(target_folder, image))\n","            print(image)\n","            print(use_method_on_image(img, method_name))\n","    \n","def use_method_on_image(image, method_name = 'dct'):\n","    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    img_gray = resizeImage(img_gray, 64)\n","    \n","    if method_name == 'dct':\n","        return dct2_method(img_gray)\n","    elif method_name == 'idct':\n","        return idct2_method(img_gray)\n","    elif method_name == 'lbp':\n","        return lbp_method(img_gray, P=8, R=1)\n","    elif method_name == 'gabor':\n","        return gabor_filters_method(img_gray, frequencies=[0.6, 1.0])\n","    \n","\n","def get_random_images_by_emotion(directory, target_class, num_of_samples=8):\n","    target_folder = os.path.join(directory, target_class)\n","    random_images = random.sample(os.listdir(target_folder), num_of_samples)\n","    return random_images\n","    #images_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n","    #get_random_images_by_emotion(images_directory, 'angry', 8)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["images_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n","images_directory = '../Data/images/train'\n","\n","#get_sample_of_feature_extractions(images_directory, 4, 'gabor')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:26:13.318603Z","iopub.status.busy":"2024-02-03T13:26:13.317890Z","iopub.status.idle":"2024-02-03T13:26:13.333880Z","shell.execute_reply":"2024-02-03T13:26:13.332821Z","shell.execute_reply.started":"2024-02-03T13:26:13.318569Z"},"trusted":true},"outputs":[],"source":["def extract_features_labels_by_method(directory, method, total_input_count = -1):\n","    features = []\n","    labels = []\n","    for emotion_folder in os.listdir(directory):\n","        emotion_folder_path = os.path.join(directory, emotion_folder)\n","        \n","        if os.path.isdir(emotion_folder_path):\n","            print(f\"Processing {emotion_folder} images...\")\n","            count = 0\n","            for image_file in tqdm(os.listdir(emotion_folder_path)):\n","                if total_input_count > 0 and count >= total_input_count:\n","                    continue\n","                count += 1\n","                image_path = os.path.join(emotion_folder_path, image_file)\n","                img = cv2.imread(image_path)\n","                features.append(use_method_on_image(img, method))\n","                label = emotion_folder\n","                labels.append(label)\n","    return features, labels"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:10:37.350847Z","iopub.status.busy":"2024-02-03T13:10:37.350477Z","iopub.status.idle":"2024-02-03T13:13:37.533772Z","shell.execute_reply":"2024-02-03T13:13:37.532814Z","shell.execute_reply.started":"2024-02-03T13:10:37.350819Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing happy images...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7164 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7164/7164 [00:01<00:00, 5757.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing sad images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4938/4938 [00:00<00:00, 6004.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing fear images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4103/4103 [00:00<00:00, 5797.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing surprise images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3205/3205 [00:00<00:00, 5737.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing neutral images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4982/4982 [00:00<00:00, 5818.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing angry images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3993/3993 [00:00<00:00, 5884.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing disgust images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 436/436 [00:00<00:00, 9114.69it/s]\n"]}],"source":["#images_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n","features_dct, labels_dct = extract_features_labels_by_method(images_directory, 'dct')"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:26:21.831650Z","iopub.status.busy":"2024-02-03T13:26:21.830974Z","iopub.status.idle":"2024-02-03T13:26:21.836158Z","shell.execute_reply":"2024-02-03T13:26:21.835174Z","shell.execute_reply.started":"2024-02-03T13:26:21.831616Z"},"trusted":true},"outputs":[],"source":["def flatten_images(images):\n","    flattened_images = []\n","    for img in images:\n","        # Flatten the image and append it to the flattened_images list\n","        flattened_images.append(img.flatten())\n","    return flattened_images"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:30:55.923089Z","iopub.status.busy":"2024-02-03T13:30:55.922317Z","iopub.status.idle":"2024-02-03T13:54:14.859817Z","shell.execute_reply":"2024-02-03T13:54:14.858924Z","shell.execute_reply.started":"2024-02-03T13:30:55.923058Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["20174 64 64\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10)</pre></div></div></div></div></div>"],"text/plain":["SVC(C=10)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# DCT MODEL\n","R =random.randint(1,88)\n","X_train_dct, X_test_dct, y_train_dct, y_test_dct = train_test_split(features_dct, labels_dct, test_size= 0.3,random_state=R)\n","\n","\n","X_train_dct_array = np.array(X_train_dct)\n","nsamples, nx, ny = X_train_dct_array.shape\n","d2_train_dataset = X_train_dct_array.reshape((nsamples,nx*ny))\n","print(nsamples, nx, ny)\n","\n","svm_clf_dct = SVC(kernel='rbf', gamma='scale', C= 10)\n","svm_clf_dct.fit(d2_train_dataset, y_train_dct)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","def train(model_type, features, labels, test_size=0.2):\n","    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=42)\n","    if model_type == 'svm':\n","        model = SVC(kernel='rbf', gamma='scale', C= 10)\n","    elif model_type == 'knn':\n","        model = KNeighborsClassifier(n_neighbors=5)\n","    elif model_type == 'rf':\n","        model = RandomForestClassifier(n_estimators=100, random_state=42)\n","     \n","    model.fit(X_train, y_train)\n","    return model, X_train, X_test, y_train, y_test\n","\n","def test(model, X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","\n","def predict_and_display(model, image_path):\n","    # Load and preprocess the image\n","    img = cv2.imread(image_path)\n","    img_processed = preprocess_image(img)  # Replace with your actual preprocessing steps\n","\n","    # Predict the expression\n","    prediction = model.predict([img_processed])\n","\n","    # Display the image and prediction\n","    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert color from BGR to RGB\n","    plt.title(f\"Predicted Expression: {prediction}\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","def preprocess_image(image):\n","    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    processed_image = resizeImage(img_gray, 64)\n","    processed_image = processed_image.flatten()\n","    return processed_image\n","\n","\n","def predict_emotion(model, image_path):\n","    img = cv2.imread(image_path)\n","    dct_feature = use_method_on_image(img, 'idct')\n","    # Model expects a 2D array as input; reshape the feature to meet this requirement\n","    dct_feature_reshaped = dct_feature.reshape(1, -1)\n","    prediction = model.predict(dct_feature_reshaped)\n","    return prediction[0]\n","\n","def output_test_on_image_list(model, images_path):\n","    for image_path in images_path:\n","        predict_and_display(model, image_path)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing happy images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7164/7164 [00:00<00:00, 174919.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing sad images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4938/4938 [00:00<00:00, 156711.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing fear images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4103/4103 [00:00<00:00, 132205.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing surprise images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3205/3205 [00:00<00:00, 104098.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing neutral images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4982/4982 [00:00<00:00, 160601.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing angry images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3993/3993 [00:00<00:00, 123495.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing disgust images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 436/436 [00:00<00:00, 16525.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["KNN Accuracy with Gabor Features: 0.31976744186046513\n"]}],"source":["features_idct, labels_idct = extract_features_labels_by_method(images_directory, 'idct', 500)\n","\n","# Convert features to a suitable shape\n","features_gabor = np.array([f.flatten() for f in features_idct])\n","model_gabor_knn, X_train_knn, X_test_knn, y_train_knn, y_test_knn = train('svm', features_gabor, labels_idct)\n","acc_knn_gabor = test(model_gabor_knn, X_test_knn, y_test_knn)\n","print(f\"KNN Accuracy with Gabor Features: {acc_knn_gabor}\")\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing happy images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7164/7164 [00:00<00:00, 160139.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing sad images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4938/4938 [00:00<00:00, 136646.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing fear images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4103/4103 [00:00<00:00, 116031.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing surprise images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3205/3205 [00:00<00:00, 96101.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing neutral images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4982/4982 [00:00<00:00, 145096.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing angry images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3993/3993 [00:00<00:00, 112084.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing disgust images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 436/436 [00:00<00:00, 19581.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing happy images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7164/7164 [00:00<00:00, 109673.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing sad images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4938/4938 [00:00<00:00, 78966.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing fear images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4103/4103 [00:00<00:00, 67611.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing surprise images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3205/3205 [00:00<00:00, 52187.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing neutral images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4982/4982 [00:00<00:00, 78237.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing angry images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3993/3993 [00:00<00:00, 64323.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing disgust images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 436/436 [00:00<00:00, 7032.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing happy images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7164/7164 [00:00<00:00, 64542.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing sad images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4938/4938 [00:00<00:00, 44633.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing fear images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4103/4103 [00:00<00:00, 37683.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing surprise images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3205/3205 [00:00<00:00, 29510.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing neutral images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4982/4982 [00:00<00:00, 43521.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing angry images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3993/3993 [00:00<00:00, 36779.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processing disgust images...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 436/436 [00:00<00:00, 4046.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["KNN Accuracy Train: 0.43392857142857144\n","KNN Accuracy Test: 0.18571428571428572\n","SVM Accuracy Train: 0.35446428571428573\n","SVM Accuracy Test: 0.2357142857142857\n","Decision Tree Accuracy Train: 0.9991071428571429\n","Decision Tree Accuracy Test: 0.18214285714285713\n","Random Forest Accuracy Train: 0.9991071428571429\n","Random Forest Accuracy Test: 0.175\n","Naive Bayes Accuracy Train: 0.45625\n","Naive Bayes Accuracy Test: 0.26071428571428573\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","features_idct, labels_idct = extract_features_labels_by_method(images_directory, 'idct', 200)\n","features_lbp, labels_idct = extract_features_labels_by_method(images_directory, 'lbp', 200)\n","features_gabor, labels_idct = extract_features_labels_by_method(images_directory, 'gabor', 200)\n","\n","features_idct = np.array([f.flatten() for f in features_idct])\n","\n","\n","features_gabor = np.array(features_gabor)\n","features_lbp = np.array(features_lbp)\n","\n","combined_features = merge_features(features_idct, features_lbp, features_gabor)\n","\n","# Örnek model listesi\n","models = {\n","    \"KNN\": KNeighborsClassifier(),\n","    \"SVM\": SVC(),\n","    \"Decision Tree\": DecisionTreeClassifier(),\n","    \"Random Forest\": RandomForestClassifier(),\n","    \"Naive Bayes\": GaussianNB(),\n","}\n","\n","# Veri setini bölme\n","X_train, X_test, y_train, y_test = train_test_split(combined_features, labels_idct, test_size=0.2, random_state=32)\n","\n","# Her modeli eğitin ve test edin\n","for name, model in models.items():\n","    model.fit(X_train, y_train)\n","    predictions_test = model.predict(X_test)\n","    predictions_train = model.predict(X_train)\n","    accuracy_test = accuracy_score(y_test, predictions_test)\n","    accuracy_train = accuracy_score(y_train, predictions_train)\n","    print(f\"{name} Accuracy Train: {accuracy_train}\")\n","    print(f\"{name} Accuracy Test: {accuracy_test}\")\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import RFE\n","from sklearn.tree import DecisionTreeClassifier\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","def feature_selection_and_normalization(features, labels):\n","    X = np.array(features)\n","    y = np.array(labels)\n","\n","    # RFE modeli\n","    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n","    rfe = rfe.fit(X, y)\n","\n","    # Seçilen özellikler\n","    selected_features = rfe.support_\n","\n","    # Filtrelenmiş özellikler\n","    X_filtered = X[:, selected_features]\n","    return X_filtered, y, selected_features\n","    scaler = StandardScaler()\n","    X_normalized = scaler.fit_transform(X_filtered)\n","\n","    return X_normalized, y, selected_features"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_normalized, y, selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_selection_and_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_idct\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 12\u001b[0m, in \u001b[0;36mfeature_selection_and_normalization\u001b[0;34m(features, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# RFE modeli\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(), n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m rfe \u001b[38;5;241m=\u001b[39m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Seçilen özellikler\u001b[39;00m\n\u001b[1;32m     15\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m rfe\u001b[38;5;241m.\u001b[39msupport_\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:249\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# RFE.estimator is not validated yet\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_selection/_rfe.py:297\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 297\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    300\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    301\u001b[0m     estimator,\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    303\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    304\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["X_normalized, y, selected_features = feature_selection_and_normalization(combined_features, labels_idct)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T13:59:01.714143Z","iopub.status.busy":"2024-02-03T13:59:01.713787Z","iopub.status.idle":"2024-02-03T13:59:01.944936Z","shell.execute_reply":"2024-02-03T13:59:01.943445Z","shell.execute_reply.started":"2024-02-03T13:59:01.714114Z"},"trusted":true},"outputs":[],"source":["predict_and_display(model, '../Data/images/train/happy/14.jpg')"]},{"cell_type":"markdown","metadata":{},"source":["# **CNN PART**"]},{"cell_type":"code","execution_count":63,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 28821 images belonging to 7 classes.\n","Found 28821 images belonging to 7 classes.\n","Epoch 1/10\n","901/901 [==============================] - 24s 27ms/step - loss: 1.6106 - accuracy: 0.3699 - val_loss: 1.4469 - val_accuracy: 0.4417\n","Epoch 2/10\n","901/901 [==============================] - 24s 27ms/step - loss: 1.3974 - accuracy: 0.4662 - val_loss: 1.2775 - val_accuracy: 0.5098\n","Epoch 3/10\n","901/901 [==============================] - 24s 27ms/step - loss: 1.2851 - accuracy: 0.5106 - val_loss: 1.1930 - val_accuracy: 0.5488\n","Epoch 4/10\n","901/901 [==============================] - 23s 26ms/step - loss: 1.1901 - accuracy: 0.5515 - val_loss: 1.0899 - val_accuracy: 0.6011\n","Epoch 5/10\n","901/901 [==============================] - 23s 26ms/step - loss: 1.0903 - accuracy: 0.5926 - val_loss: 0.9790 - val_accuracy: 0.6447\n","Epoch 6/10\n","901/901 [==============================] - 23s 26ms/step - loss: 0.9929 - accuracy: 0.6297 - val_loss: 0.8727 - val_accuracy: 0.6864\n","Epoch 7/10\n","901/901 [==============================] - 26s 28ms/step - loss: 0.9040 - accuracy: 0.6662 - val_loss: 0.7644 - val_accuracy: 0.7291\n","Epoch 8/10\n","901/901 [==============================] - 25s 27ms/step - loss: 0.8089 - accuracy: 0.7043 - val_loss: 0.6821 - val_accuracy: 0.7672\n","Epoch 9/10\n","901/901 [==============================] - 23s 26ms/step - loss: 0.7356 - accuracy: 0.7291 - val_loss: 0.5937 - val_accuracy: 0.7968\n","Epoch 10/10\n","901/901 [==============================] - 24s 26ms/step - loss: 0.6449 - accuracy: 0.7650 - val_loss: 0.5221 - val_accuracy: 0.8210\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Veri seti yolu\n","train_directory = '../Data/images/train'\n","test_directory = '../Data/images/train'\n","\n","# Veri artırma ve ön işleme\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Eğitim ve test veri setlerini yükleme\n","train_generator = train_datagen.flow_from_directory(\n","    train_directory,\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='categorical')\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_directory,\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='categorical')\n","\n","# CNN modeli oluşturma\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    MaxPooling2D(2, 2),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dense(7, activation='softmax')  # Sınıf sayısına göre ayarlayın\n","])\n","\n","# Modeli derleme\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Modeli eğitme\n","history = model.fit(train_generator, epochs=10, validation_data=test_generator)\n","\n","\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# save the model\n","save_model(model, 'model.h5')"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"]}],"source":["loaded_model = load_model('model.h5')\n","\n","print(train_generator.class_indices)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 10ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 8ms/step\n","1/1 [==============================] - 0s 9ms/step\n","[2, 1, 8, 2, 1, 2, 34]\n"]}],"source":["emotion_label_list = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n","import numpy as np\n","import cv2\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","\n","def preprocess_image(image_path, target_size=(64, 64)):\n","    # Görüntüyü yükle ve boyutlandır\n","    img = load_img(image_path, target_size=target_size)\n","    img_array = img_to_array(img)  # Görüntüyü array'e dönüştür\n","    img_array = np.expand_dims(img_array, axis=0)  # Modelin beklediği şekle getir\n","    return img_array / 255.0  # Normalize et\n","\n","def predict_emotion(model, image_path):\n","    processed_image = preprocess_image(image_path)\n","    prediction = model.predict(processed_image)\n","    return prediction\n","\n","# Modelinizi yükleyin (eğer henüz yüklenmediyse)\n","# model = tf.keras.models.load_model('model_yolu')\n","\n","# # Tahmin yap\n","# image_path = '../Data/images/train/angry/126.jpg'\n","# prediction = predict_emotion(model, image_path)\n","\n","# # Prediction'ı yorumlayın\n","# # Örneğin, en yüksek skorlu sınıfı bulabilirsiniz:\n","# predicted_class = np.argmax(prediction, axis=1)\n","# print(f\"Predicted class: {predicted_class}\")\n","# for key, value in emotion_label_list.items():\n","#     if value == predicted_class:\n","#         print(key)\n","\n","# loss, accuracy = model.evaluate(train_generator)\n","# print(f\"Model Accuracy: {accuracy}\")\n","\n","# for random 50 images in ../Data/images/train/angry predict the emotion\n","predicted_classes = [0,0,0,0,0,0,0]\n","for image in get_random_images_by_emotion('../Data/images/validation', 'surprise', 50):\n","    image_path = f'../Data/images/validation/surprise/{image}'\n","    prediction = predict_emotion(model, image_path)\n","    predicted_class = np.argmax(prediction, axis=1)\n","    predicted_classes[int(predicted_class)] += 1\n","\n","print(predicted_classes)\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'GaussianNB' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Önceki CNN modeliniz\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cnn_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# CNN için giriş\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# CNN modeli\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Geleneksel özellikler için giriş (örneğin, 100 özellik)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m traditional_features_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,))\n","\u001b[0;31mTypeError\u001b[0m: 'GaussianNB' object is not callable"]}],"source":["from tensorflow.keras.layers import Input, concatenate\n","from tensorflow.keras.models import Model\n","\n","# Önceki CNN modeliniz\n","cnn_input = Input(shape=(64, 64, 3))  # CNN için giriş\n","cnn_model = model(cnn_input)  # CNN modeli\n","\n","# Geleneksel özellikler için giriş (örneğin, 100 özellik)\n","traditional_features_input = Input(shape=(100,))\n","\n","# Özellikleri birleştir\n","combined_features = concatenate([cnn_model, traditional_features_input])\n","\n","# Tam bağlantılı katmanlar\n","x = Dense(64, activation='relu')(combined_features)\n","output = Dense(7, activation='softmax')(x)  # Sınıf sayısına göre ayarlayın\n","\n","# Yeni model\n","new_model = Model(inputs=[cnn_input, traditional_features_input], outputs=output)\n","\n","# Modeli derle\n","new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 28821 images belonging to 7 classes.\n","Found 28821 images belonging to 7 classes.\n","901/901 [==============================] - 135s 150ms/step\n","901/901 [==============================] - 132s 146ms/step\n"]},{"ename":"NameError","evalue":"name 'Sequential' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m test_features \u001b[38;5;241m=\u001b[39m test_features\u001b[38;5;241m.\u001b[39mreshape(test_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Build the CNN model\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[1;32m     84\u001b[0m     Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(train_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[1;32m     85\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     86\u001b[0m     Dense(\u001b[38;5;241m7\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     87\u001b[0m ])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"]}],"source":["# import tensorflow as tf\n","# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# # Veri seti yolu\n","# train_directory = '../Data/images/train'\n","# test_directory = '../Data/images/train'\n","\n","# # Veri artırma ve ön işleme\n","# train_datagen = ImageDataGenerator(rescale=1./255)\n","# test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# # Eğitim ve test veri setlerini yükleme\n","# train_generator = train_datagen.flow_from_directory(\n","#     train_directory,\n","#     target_size=(64, 64),\n","#     batch_size=32,\n","#     class_mode='categorical')\n","\n","# test_generator = test_datagen.flow_from_directory(\n","#     test_directory,\n","#     target_size=(64, 64),\n","#     batch_size=32,\n","#     class_mode='categorical')\n","\n","# # CNN modeli oluşturma\n","# model = Sequential([\n","#     Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","#     MaxPooling2D(2, 2),\n","#     Conv2D(64, (3, 3), activation='relu'),\n","#     MaxPooling2D(2, 2),\n","#     Flatten(),\n","#     Dense(64, activation='relu'),\n","#     Dense(7, activation='softmax')  # Sınıf sayısına göre ayarlayın\n","# ])\n","\n","# # Modeli derleme\n","# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# # Modeli eğitme\n","# history = model.fit(train_generator, epochs=10, validation_data=test_generator)\n","\n","\n","\n","\n","# Import the necessary libraries\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","# Load the VGG16 model without the top layers\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n","\n","# Veri seti yolu\n","train_directory = '../Data/images/train'\n","test_directory = '../Data/images/train'\n","\n","# Veri artırma ve ön işleme\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Eğitim ve test veri setlerini yükleme\n","train_generator = train_datagen.flow_from_directory(\n","    train_directory,\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='categorical')\n","\n","test_generator = test_datagen.flow_from_directory(\n","    test_directory,\n","    target_size=(64, 64),\n","    batch_size=32,\n","    class_mode='categorical')\n","\n","# Extract features from the images using the VGG16 model\n","train_features = base_model.predict(train_generator)\n","test_features = base_model.predict(test_generator)\n","\n","# Preprocess the extracted features\n","train_features = train_features.reshape(train_features.shape[0], -1)\n","test_features = test_features.reshape(test_features.shape[0], -1)\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"ename":"ValueError","evalue":"in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 7) are incompatible\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model using the extracted features\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/var/folders/4s/9lp6822x67n5q20pk553klcw0000gn/T/__autograph_generated_file33ol5yrl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 7) are incompatible\n"]}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","# Build the CNN model\n","model = Sequential([\n","    Dense(256, activation='relu', input_shape=(train_features.shape[1],)),\n","    Dropout(0.5),\n","    Dense(7, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model using the extracted features\n","history = model.fit(train_features, train_generator.labels, epochs=10, validation_data=(test_features, test_generator.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":99505,"sourceId":234911,"sourceType":"datasetVersion"},{"datasetId":4394533,"sourceId":7545898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
