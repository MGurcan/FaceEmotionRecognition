{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, path):\n",
    "    #joblib.dump(svm_clf_dct, 'svm_dct_model.pkl')\n",
    "    joblib.dump(model, path)\n",
    "    \n",
    "def load_model(path):\n",
    "    # Load the model\n",
    "    #svm_clf_dct_local = joblib.load('/kaggle/input/svm-dct-model/svm_dct_model.pkl')\n",
    "    return(joblib.load(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIC METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm  # for progress bar\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from imutils import paths\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from skimage.filters import gabor\n",
    "from scipy.spatial.qhull import QhullError\n",
    "from scipy import spatial\n",
    "spatial.QhullError = QhullError\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colortogray(im):\n",
    "    image = cv2.imread(im)\n",
    "    imgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return imgray\n",
    "\n",
    "def resizeImage(image, size):\n",
    "    return cv2.resize(image, (size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_labels(directory):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for emotion_folder in os.listdir(directory):\n",
    "        emotion_folder_path = os.path.join(directory, emotion_folder)\n",
    "        \n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(emotion_folder_path):\n",
    "            print(f\"Processing {emotion_folder} images...\")\n",
    "        for image_file in tqdm(os.listdir(emotion_folder_path)):\n",
    "                image_path = os.path.join(emotion_folder_path, image_file)\n",
    "                image = colortogray(image_path)\n",
    "                image = resizeImage(image, 64)\n",
    "\n",
    "                fd1 = fd1 =  hog(image, orientations=7, pixels_per_cell=(8, 8),cells_per_block=(4, 4),block_norm= 'L2-Hys' ,transform_sqrt = False)\n",
    "                label = emotion_folder\n",
    "                labels.append(label)\n",
    "                features.append(fd1)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "# implement 2D DCT\n",
    "def dct2_method(a):\n",
    "    return dct(dct(a.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "# implement 2D IDCT\n",
    "def idct2_method(a):\n",
    "    return idct(idct(a.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "def lbp_method(image, P, R):\n",
    "    lbp = local_binary_pattern(image, P, R, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, P + 3), range=(0, P + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    return hist\n",
    "\n",
    "def gabor_filters_method(image, frequencies):\n",
    "    responses = []\n",
    "    for frequency in frequencies:\n",
    "        real, imag = gabor(image, frequency=frequency)\n",
    "        magnitude = np.sqrt(real**2 + imag**2)\n",
    "        responses.append(magnitude.flatten())  # Flatten each response\n",
    "    combined_response = np.concatenate(responses, axis=0)  # Concatenate flattened responses\n",
    "    return combined_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY METHODS\n",
    "\n",
    "import math\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "def view_random_images(directory, target_class, num_of_samples=8):\n",
    "  target_folder = os.path.join(directory, target_class)\n",
    "\n",
    "  random_images = random.sample(os.listdir(target_folder), num_of_samples)\n",
    "  print(random_images)\n",
    "  # Create a subplot to display multiple images\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  for i, image in enumerate(random_images):\n",
    "    plt.subplot(math.ceil(num_of_samples / 4), 4, i + 1)  # 2 rows, 4 columns\n",
    "    img = cv2.imread(os.path.join(target_folder, image))\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{target_class} - {i + 1}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def get_sample_of_images(directory, num_of_samples = 8):    \n",
    "    for emotion in emotions:\n",
    "        view_random_images(directory, emotion, num_of_samples)\n",
    "\n",
    "def get_sample_of_feature_extractions(directory, num_of_samples = 8, method_name = 'dct'):\n",
    "    for emotion_target in emotions:\n",
    "        target_folder = os.path.join(directory, emotion_target)\n",
    "        random_images = random.sample(os.listdir(target_folder), num_of_samples)\n",
    "        print(emotion_target)\n",
    "        for i, image in enumerate(random_images):\n",
    "            img = cv2.imread(os.path.join(target_folder, image))\n",
    "            print(image)\n",
    "            print(use_method_on_image(img, method_name))\n",
    "    \n",
    "def use_method_on_image(image, method_name = 'dct'):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_gray = resizeImage(img_gray, 64)\n",
    "    \n",
    "    if method_name == 'dct':\n",
    "        return dct2_method(img_gray)\n",
    "    elif method_name == 'idct':\n",
    "        return idct2_method(img_gray)\n",
    "    elif method_name == 'lbp':\n",
    "        return lbp_method(img_gray, P=8, R=1)\n",
    "    elif method_name == 'gabor':\n",
    "        return gabor_filters_method(img_gray, frequencies=[0.6, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n",
    "#get_sample_of_images(images_directory)\n",
    "\n",
    "get_sample_of_feature_extractions(images_directory, 4, 'gabor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_labels_by_method(directory, method, total_input_count = -1):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for emotion_folder in os.listdir(directory):\n",
    "        emotion_folder_path = os.path.join(directory, emotion_folder)\n",
    "        \n",
    "        if os.path.isdir(emotion_folder_path):\n",
    "            print(f\"Processing {emotion_folder} images...\")\n",
    "            count = 0\n",
    "            for image_file in tqdm(os.listdir(emotion_folder_path)):\n",
    "                if total_input_count > 0 and count >= total_input_count / 3:\n",
    "                    continue\n",
    "                count += 1\n",
    "                image_path = os.path.join(emotion_folder_path, image_file)\n",
    "                img = cv2.imread(image_path)\n",
    "                features.append(use_method_on_image(img, method))\n",
    "                label = emotion_folder\n",
    "                labels.append(label)\n",
    "    return features, labels\n",
    "def extract_features_labels_all_methods(directory, method_list, total_input_count = -1):\n",
    "    all_features = []\n",
    "    for method_name in method_list:\n",
    "        extract_features_labels_by_method(directory, method_name, total_input_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n",
    "features_dct, labels_dct = extract_features_labels_by_method(images_directory, 'dct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_images(images):\n",
    "    flattened_images = []\n",
    "    for img in images:\n",
    "        # Flatten the image and append it to the flattened_images list\n",
    "        flattened_images.append(img.flatten())\n",
    "    return flattened_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCT MODEL\n",
    "R =random.randint(1,88)\n",
    "X_train_dct, X_test_dct, y_train_dct, y_test_dct = train_test_split(features_dct, labels_dct, test_size= 0.3,random_state=R)\n",
    "\n",
    "\n",
    "X_train_dct_array = np.array(X_train_dct)\n",
    "nsamples, nx, ny = X_train_dct_array.shape\n",
    "d2_train_dataset = X_train_dct_array.reshape((nsamples,nx*ny))\n",
    "print(nsamples, nx, ny)\n",
    "\n",
    "svm_clf_dct = SVC(kernel='rbf', gamma='scale', C= 10)\n",
    "svm_clf_dct.fit(d2_train_dataset, y_train_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_display(model, image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_processed = preprocess_image(img)  # Replace with your actual preprocessing steps\n",
    "\n",
    "    # Predict the expression\n",
    "    prediction = model.predict([img_processed])[0]\n",
    "\n",
    "    # Display the image and prediction\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert color from BGR to RGB\n",
    "    plt.title(f\"Predicted Expression: {prediction}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    processed_image = resizeImage(img_gray, 64)\n",
    "    processed_image = processed_image.flatten()\n",
    "    return processed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_display(svm_clf_dct_local, '/kaggle/input/face-expression-recognition-dataset/images/train/angry/0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Veri seti yolu\n",
    "train_directory = '/kaggle/input/face-expression-recognition-dataset/images/train'\n",
    "test_directory = '/kaggle/input/face-expression-recognition-dataset/images/validation'\n",
    "\n",
    "# Veri artırma ve ön işleme\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Eğitim ve test veri setlerini yükleme\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# CNN modeli oluşturma\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # Sınıf sayısına göre ayarlayın\n",
    "])\n",
    "\n",
    "# Modeli derleme\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modeli eğitme\n",
    "history = model.fit(train_generator, epochs=10, validation_data=test_generator)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
